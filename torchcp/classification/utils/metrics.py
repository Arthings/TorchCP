# Copyright (c) 2023-present, SUSTech-ML.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#
import numpy as np
from typing import Any

from torchcp.utils.registry import Registry

METRICS_REGISTRY_CLASSIFICATION = Registry("METRICS")


#########################################
# Marginal coverage metric
#########################################

@METRICS_REGISTRY_CLASSIFICATION.register()
def coverage_rate(prediction_sets, labels, coverage_type="default", num_classes=None):
    """
    The metric for empirical coverage.
    
    Empirically Validating Conformal Prediction on Modern Vision Architectures Under Distribution Shift and Long-tailed Data (Kasa et al., 2023)
    paper: https://arxiv.org/abs/2307.01088
    
    Args:
        prediction_sets (List): the prediction sets generated by CP algorithms.
        labels (list): the ground-truth label of each samples.
        coverage_type (str, optional): the type of coverage rate. Defaults to "default". Options are 'default' (the marginal coverage rate), 'macro' (the average coverage rate of all classes).
        num_classes (_type_, optional): the number of classes. When coverage_type == 'macro", you must define the number of classes.

    Returns:
        float: the empirical coverage rate.
    """
    assert len(prediction_sets) > 0, "The number of prediction set must be greater than 0."
    labels = labels.cpu()
    cvg = 0

    if coverage_type == "macro":
        assert (num_classes != None), "Macro Coverage metric needs the number of classes."
        rate_classes = []
        for k in range(num_classes):
            idx = np.where(labels == k)[0]
            selected_preds = [prediction_sets[i] for i in idx]
            if len(labels[labels == k]) != 0:
                rate_classes.append(coverage_rate(selected_preds, labels[labels == k]))
            else:
                # If there is no the "k" class in the "labels", we skip the calculation of this class.
                continue
        cvg = np.mean(rate_classes)
    else:
        for index, ele in enumerate(zip(prediction_sets, labels)):
            if ele[1] in ele[0]:
                cvg += 1
        cvg = cvg / len(prediction_sets)
    return cvg


@METRICS_REGISTRY_CLASSIFICATION.register()
def average_size(prediction_sets, labels):
    assert len(prediction_sets) > 0, "The number of prediction set must be greater than 0."

    labels = labels.cpu()
    avg_size = 0
    for index, ele in enumerate(prediction_sets):
        avg_size += len(ele)
    return avg_size / len(prediction_sets)


#########################################
# Conditional coverage metric
#########################################

@METRICS_REGISTRY_CLASSIFICATION.register()
def CovGap(prediction_sets, labels, alpha, num_classes, shot_idx=None):
    """
    Computing the average class-conditional coverage gap.

    Empirically Validating Conformal Prediction on Modern Vision Architectures Under Distribution Shift and Long-tailed Data (Ding et al., 2023)
    paper: https://neurips.cc/virtual/2023/poster/70548
    
    Args:
        prediction_sets (List):  the prediction sets generated by CP algorithms.
        labels (list): the ground-truth label of each samples.
        alpha (Float): the user-guided confidence level.
        num_classes (Int): the number of classes.
        shot_idx (List, optional): the indices of classes that need to compute the coverage gap. Defaults to None.

    Returns:
        Float: the average class-conditional coverage gap.
    """
    assert len(prediction_sets) > 0, "The number of prediction sets must be greater than 0."

    labels = labels.cpu()
    cls_coverage = []
    for k in range(num_classes):
        idx = np.where(labels == k)[0]
        selected_preds = [prediction_sets[i] for i in idx]

        if len(labels[labels == k]) != 0:
            the_coverage = coverage_rate(selected_preds, labels[labels == k])
        else:
            the_coverage = 0
        cls_coverage.append(the_coverage)

    cls_coverage = np.array(cls_coverage)
    overall_covgap = np.mean(np.abs(cls_coverage - (1 - alpha))) * 100

    if shot_idx == None:
        return overall_covgap
    covgaps = [overall_covgap]
    for shot in shot_idx:
        shot_covgap = np.mean(np.abs(cls_coverage[shot] - (1 - alpha))) * 100
        covgaps.append(shot_covgap)

    return covgaps


@METRICS_REGISTRY_CLASSIFICATION.register()
def VioClasses(prediction_sets, labels, alpha, num_classes):
    """
    The number of violated classes.

    Empirically Validating Conformal Prediction on Modern Vision Architectures Under Distribution Shift and Long-tailed Data (Kasa et al., 2023)
    paper: https://arxiv.org/abs/2307.01088
    
    Args:
        prediction_sets (List): the prediction sets generated by CP algorithms.
        labels (list): the ground-truth label of each samples.
        alpha (Float): the user-guided confidence level.
        num_classes (Int): the number of classes.
    
    Returns:
        Int: the number of classes with violated coverage.
    """
    labels = labels.cpu()
    violation_nums = 0
    for k in range(num_classes):
        if len(labels[labels == k]) == 0:
            violation_nums += 1
        else:
            idx = np.where(labels == k)[0]
            selected_preds = [prediction_sets[i] for i in idx]
            if coverage_rate(selected_preds, labels[labels == k]) < 1 - alpha:
                violation_nums += 1
    return violation_nums


@METRICS_REGISTRY_CLASSIFICATION.register()
def DiffViolation(logits, prediction_sets, labels, alpha,
                  strata_diff=[[1, 1], [2, 3], [4, 6], [7, 10], [11, 100], [101, 1000]]):
    """
    Difficulty-stratified coverage violation

    Regularized Adaptive Prediction Sets (Angelopoulos et al., 2020)
    paper : https://arxiv.org/abs/2009.14193
    
    
    Args:
        logits (Array): the predicted logits.
        prediction_sets (List): the prediction sets generated by CP algorithms.
        labels (list): the ground-truth label of each samples.
        alpha (Float): the user-guided confidence level.
        strata_diff (List): a coarse partitioning of the possible difficulties.

    Returns:
        2-tuple: (the difficulty-stratified coverage violation, the number of samples, the empirical coverage and size of each difficulty).
    """
    assert isinstance(strata_diff, list), "strata_diff must be a list."

    labels = labels.cpu()
    logits = logits.cpu()
    correct_array = np.zeros(len(labels))
    size_array = np.zeros(len(labels))
    topk = []
    for index, ele in enumerate(logits):
        I = ele.argsort(descending=True)
        target = labels[index]
        topk.append(np.where((I - target.view(-1, 1).numpy()) == 0)[1] + 1)
        correct_array[index] = 1 if labels[index] in prediction_sets[index] else 0
        size_array[index] = len(prediction_sets[index])
    topk = np.concatenate(topk)

    ccss_diff = {}
    diff_violation = -1

    for stratum in strata_diff:
        temp_index = np.argwhere((topk >= stratum[0]) & (topk <= stratum[1]))
        ccss_diff[str(stratum)] = {}
        ccss_diff[str(stratum)]['cnt'] = len(temp_index)
        if len(temp_index) == 0:
            ccss_diff[str(stratum)]['cvg'] = 0
            ccss_diff[str(stratum)]['sz'] = 0
        else:
            temp_index = temp_index[:, 0]
            cvg = np.round(np.mean(correct_array[temp_index]), 3)
            sz = np.round(np.mean(size_array[temp_index]), 3)

            ccss_diff[str(stratum)]['cvg'] = cvg
            ccss_diff[str(stratum)]['sz'] = sz
            stratum_violation = max(0, (1 - alpha) - cvg)
            diff_violation = max(diff_violation, stratum_violation)

    return diff_violation, ccss_diff


@METRICS_REGISTRY_CLASSIFICATION.register()
def SSCV(prediction_sets, labels, alpha, stratified_size=[[0, 1], [2, 3], [4, 10], [11, 100], [101, 1000]]):
    """
    Size-stratified coverage violation (SSCV).
    
    Regularized Adaptive Prediction Sets (Angelopoulos et al., 2020)
    paper : https://arxiv.org/abs/2009.14193
    
    Args:
        prediction_sets (List): the prediction sets generated by CP algorithms.
        labels (list): the ground-truth label of each samples.
        alpha (Float): the user-guided confidence level.
        stratified_size (List): a coarse partitioning of the possible set sizes.
    
    Returns:
        Int: the value of SSCV.
    
    """
    labels = labels.cpu()
    size_array = np.zeros(len(labels))
    correct_array = np.zeros(len(labels))
    for index, ele in enumerate(prediction_sets):
        size_array[index] = len(ele)
        correct_array[index] = 1 if labels[index] in ele else 0

    sscv = -1
    for stratum in stratified_size:
        temp_index = np.argwhere((size_array >= stratum[0]) & (size_array <= stratum[1]))
        if len(temp_index) > 0:
            stratum_violation = abs((1 - alpha) - np.mean(correct_array[temp_index]))
            sscv = max(sscv, stratum_violation)
    return sscv

import torch
@METRICS_REGISTRY_CLASSIFICATION.register()
def WSC(X, S, y, delta=0.1, M=1000, test_size=0.75, random_state=2020, verbose=True):
    """
    WSC
    
     Classification with Valid and Adaptive Coverage (Romano et al., 2020)
     paper : https://proceedings.neurips.cc/paper/2020/hash/244edd7e85dc81602b7615cd705545f5-Abstract.html
     Code: https://github.com/msesia/arc/tree/d80d27519f18b11e7feaf8cf0da8827151af9ce3

    
     Args:
         features : the features of input.
         labels (List): the ground-truth label of each samples.
         prediction_sets (List): the prediction sets generated by CP algorithms.
         stratified_size (List): a coarse partitioning of the possible set sizes.
    
     Returns:
         Float: the value of unbiased WSV.
    
    """
    def wsc_vab(X, y, S, v, a, b):
        n = len(y)
        cover = np.array([y[i] in S[i] for i in range(n)])
        z = np.dot(X,v)
        idx = np.where((z>=a)*(z<=b))
        coverage = np.mean(cover[idx])
        return coverage
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test, S_train, S_test = train_test_split(X, y, S, test_size=test_size,
                                                                        random_state=random_state)
    # Find adversarial parameters
    wsc_star, v_star, a_star, b_star = calWSC(X_train, y_train, S_train, delta=delta, M=M, random_state=random_state, verbose=verbose)
    # Estimate coverage
    coverage = wsc_vab(X_test, y_test, S_test, v_star, a_star, b_star)
    return coverage
    
    
def calWSC(X, y, S, delta=0.1, M=1000, random_state=2020, verbose=True):
    rng = np.random.default_rng(random_state)

    def wsc_v(X, y, S, delta, v):
        n = len(y)
        cover = np.array([y[i] in S[i] for i in range(n)])
        z = np.dot(X,v)
        # Compute mass
        z_order = np.argsort(z)
        z_sorted = z[z_order]
        cover_ordered = cover[z_order]
        ai_max = int(np.round((1.0-delta)*n))
        ai_best = 0
        bi_best = n-1
        cover_min = 1
        for ai in np.arange(0, ai_max):
            bi_min = np.minimum(ai+int(np.round(delta*n)),n-1)
            coverage = np.cumsum(cover_ordered[ai:n]) / np.arange(1,n-ai+1)
            coverage[np.arange(0,bi_min-ai)]=1
            bi_star = ai+np.argmin(coverage)
            cover_star = coverage[bi_star-ai]
            if cover_star < cover_min:
                ai_best = ai
                bi_best = bi_star
                cover_min = cover_star
        return cover_min, z_sorted[ai_best], z_sorted[bi_best]

    def sample_sphere(n, p):
        v = rng.normal(size=(p, n))
        v /= np.linalg.norm(v, axis=0)
        return v.T

    V = sample_sphere(M, p=X.shape[1])
    wsc_list = [[]] * M
    a_list = [[]] * M
    b_list = [[]] * M
    from tqdm import tqdm
    if verbose:
        for m in tqdm(range(M)):
            wsc_list[m], a_list[m], b_list[m] = wsc_v(X, y, S, delta, V[m])

    else:
        for m in range(M):
            wsc_list[m], a_list[m], b_list[m] = wsc_v(X, y, S, delta, V[m])                
        
    idx_star = np.argmin(np.array(wsc_list))
    a_star = a_list[idx_star]
    b_star = b_list[idx_star]
    v_star = V[idx_star]
    wsc_star = wsc_list[idx_star]
    return wsc_star, v_star, a_star, b_star


        
# def WSC(features, prediction_sets, labels , delta=0.1, M=1000, test_size=0.75, random_state=2020, verbose= False):
#     """
#     Size-stratified coverage violation (SSCV).
    
#     Classification with Valid and Adaptive Coverage (Romano et al., 2020)
#     paper : https://proceedings.neurips.cc/paper/2020/hash/244edd7e85dc81602b7615cd705545f5-Abstract.html
#     Code: https://github.com/msesia/arc/tree/d80d27519f18b11e7feaf8cf0da8827151af9ce3

    
#     Args:
#         features : the features of input.
#         labels (List): the ground-truth label of each samples.
#         prediction_sets (List): the prediction sets generated by CP algorithms.
#         stratified_size (List): a coarse partitioning of the possible set sizes.
    
#     Returns:
#         Float: the value of unbiased WSV.
    
#     """
#     coverage_vector = torch.tensor([labels[i] in prediction_sets[i] for i in range(len(prediction_sets))])
#     X_train, X_test, cover_train, cover_test = train_test_split(features, coverage_vector, test_size=test_size,
#                                                                         random_state=random_state)
#     # Find adversarial parameters
#     wsc_star, v_star, a_star, b_star = calWSC(X_train, cover_train, delta=delta, M=M, random_state=random_state, verbose =  verbose)
#     # Estimate coverage
#     coverage = wsc_vab(X_test, cover_test, v_star, a_star, b_star)
#     return coverage

# def wsc_vab(X, cover, v, a, b):
#     z = torch.matmul(X, v)
#     idx = torch.where((z >= a) & (z <= b))[0]
#     coverage = cover[idx].float().mean().item()
    
#     return coverage

# def calWSC(X, cover, delta=0.1, M=1000, random_state=2020, verbose=False):
#         rng = torch.Generator().manual_seed(random_state)
#         V = sample_sphere(M, p=X.shape[1])
#         wsc_list = torch.empty(M)
#         a_list = torch.empty(M)
#         b_list = torch.empty(M)

#         if verbose:
#             for m in tqdm(range(M)):
#                 wsc_list[m], a_list[m], b_list[m] = wsc_v(X, cover, delta, V[m])   
#         else:
#             for m in range(M):
#                 wsc_list[m], a_list[m], b_list[m] = wsc_v(X, cover, delta, V[m])                
            
#         idx_star = torch.argmin(wsc_list).item()
#         a_star = a_list[idx_star].item()
#         b_star = b_list[idx_star].item()
#         v_star = V[idx_star]
#         wsc_star = wsc_list[idx_star].item()
#         return wsc_star, v_star, a_star, b_star
    
# def sample_sphere(n, p):
#     V = torch.randn(n, p)
#     V = V / V.norm(dim=1, keepdim=True)
#     return V

# from tqdm import tqdm
# def wsc_v(X, cover, delta, v):
#     n = X.shape[0]
#     z = torch.matmul(X, v)
#     z_order = torch.argsort(z)
#     z_sorted = z[z_order]
#     cover_ordered = cover[z_order]
#     ai_max = int(np.round((1.0 - delta) * n))
#     ai_best = 0
#     bi_best = n - 1
#     cover_min = 1

#     for ai in range(ai_max):
#         bi_min = min(ai + int(np.round(delta * n)), n - 1)
#         coverage = torch.cumsum(cover_ordered[ai:n], dim=0) / torch.arange(1, n - ai + 1, dtype=torch.float32)
#         coverage[:bi_min - ai + 1] = 1
#         bi_star = ai + torch.argmin(coverage).item()
#         cover_star = coverage[bi_star - ai].item()
#         if cover_star < cover_min:
#             ai_best = ai
#             bi_best = bi_star
#             cover_min = cover_star

#     return cover_min, z_sorted[ai_best].item(), z_sorted[bi_best].item()
        
        
# def train_test_split(features, coverage_vector, test_size=0.25, random_state=None):
#     if random_state is not None:
#         torch.manual_seed(random_state)
    
#     dataset_size = len(features)
#     indices = torch.randperm(dataset_size)
    
#     test_size = int(dataset_size * test_size)
    
#     test_indices = indices[:test_size]
#     train_indices = indices[test_size:]
    
#     X_train = features[train_indices]
#     cover_train = coverage_vector[train_indices]
#     X_test = features[test_indices]
#     cover_test = coverage_vector[test_indices]
    
#     return X_train, X_test, cover_train, cover_test


class Metrics:

    def __call__(self, metric) -> Any:
        if metric not in METRICS_REGISTRY_CLASSIFICATION.registered_names():
            raise NameError(f"The metric: {metric} is not defined in TorchCP.")
        return METRICS_REGISTRY_CLASSIFICATION.get(metric)
