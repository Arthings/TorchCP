# Copyright (c) 2023-present, SUSTech-ML.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#
import numpy as np
from typing import Any
import torch

from torchcp.utils.registry import Registry

METRICS_REGISTRY_CLASSIFICATION = Registry("METRICS")


#########################################
# Marginal coverage metric
#########################################

@METRICS_REGISTRY_CLASSIFICATION.register()
def coverage_rate(prediction_sets, labels, coverage_type="default", num_classes=None):
    """
    The metric for empirical coverage.
    
    Args:
        prediction_sets (list): the prediction sets generated by CP algorithms.
        labels (torch.Tensor): the ground-truth label of each samples.
        coverage_type (str, optional): the type of coverage rate. Defaults to "default". Options are 'default' (the marginal coverage rate), 'macro' (the average coverage rate of all classes).
        num_classes (int, optional): the number of classes. When coverage_type == 'macro", you must define the number of classes.

    Returns:
        float: the empirical coverage rate.
    """
    if len(prediction_sets) != len(labels):
        raise ValueError("The number of prediction sets must be equal to the number of labels.")
    
    if coverage_type not in ["default", "macro"]:
        raise ValueError("coverage_type must be 'default' or 'macro'.")
      
    labels = labels.cpu()
    cvg = 0

    if coverage_type == "macro":
        if num_classes is None:
            raise ValueError("When coverage_type is 'macro', you must define the number of classes.")
        class_coverage = torch.zeros(num_classes, dtype=torch.float32)
        class_counts = torch.zeros(num_classes, dtype=torch.float32)
        for label, pred_set in zip(labels, prediction_sets):
            class_counts[label] += 1
            if label in pred_set:
                class_coverage[label] += 1
        class_coverage_rate = class_coverage / class_counts
        cvg = class_coverage_rate.mean().item()
    else:
        covered = torch.tensor([label in pred_set for label, pred_set in zip(labels, prediction_sets)], dtype=torch.float32)
        cvg = covered.mean().item()
    return cvg


@METRICS_REGISTRY_CLASSIFICATION.register()
def average_size(prediction_sets, labels=None):
    labels = labels.cpu()
    avg_size = 0
    for index, ele in enumerate(prediction_sets):
        avg_size += len(ele)
    return avg_size / len(prediction_sets)


#########################################
# Conditional coverage metric
#########################################

@METRICS_REGISTRY_CLASSIFICATION.register()
def CovGap(prediction_sets, labels, alpha, num_classes, shot_idx=None):
    """
    The average class-conditional coverage gap.

    Paper: Class-Conditional Conformal Prediction with Many Classes (Ding et al., 2023)
    Link: https://neurips.cc/virtual/2023/poster/70548
    
    Args:
        prediction_sets (list):  the prediction sets generated by CP algorithms.
        labels (torch.Tensor): the ground-truth label of each samples.
        alpha (float): the user-guided confidence level.
        num_classes (int): the number of classes.
        shot_idx (list, optional): the indices of classes that need to compute the coverage gap. Defaults to None.

    Returns:
        Float: the average class-conditional coverage gap.
    """
    if len(prediction_sets) != len(labels):
        raise ValueError("The number of prediction sets must be equal to the number of labels.")

    labels = labels.cpu()
    cls_coverage = torch.zeros(num_classes, dtype=torch.float32)
    class_counts = torch.zeros(num_classes, dtype=torch.float32)
    
    for label, pred_set in zip(labels, prediction_sets):
        class_counts[label] += 1
        if label in pred_set:
            cls_coverage[label] += 1
    
    cls_coverage_rate = cls_coverage / class_counts
    if shot_idx is not None:
        cls_coverage_rate = cls_coverage_rate[shot_idx]
    
    overall_covgap = torch.mean(torch.abs(cls_coverage_rate - (1 - alpha))) * 100
    return overall_covgap


@METRICS_REGISTRY_CLASSIFICATION.register()
def VioClasses(prediction_sets, labels, alpha, num_classes):
    """
    The number of violated classes.

    Paper: Empirically Validating Conformal Prediction on Modern Vision Architectures Under Distribution Shift and Long-tailed Data (Kasa et al., 2023)
    Link: https://arxiv.org/abs/2307.01088
    
    Args:
        prediction_sets (list): the prediction sets generated by CP algorithms.
        labels (torch.Tensor): the ground-truth label of each samples.
        alpha (float): the user-guided confidence level.
        num_classes (int): the number of classes.
    
    Returns:
        Int: the number of classes with violated coverage.
    """
    if len(prediction_sets) != len(labels):
        raise ValueError("The number of prediction sets must be equal to the number of labels.")
    
    labels = labels.cpu()
    violation_nums = 0
    for k in range(num_classes):
        class_labels = labels == k
        if class_labels.sum() == 0:
            continue
        else:
            idx = torch.where(class_labels)[0]
            selected_preds = [prediction_sets[i] for i in idx]
            if coverage_rate(selected_preds, labels[class_labels]) < 1 - alpha:
                violation_nums += 1
    return violation_nums


@METRICS_REGISTRY_CLASSIFICATION.register()
def DiffViolation(logits, prediction_sets, labels, alpha,
                  strata_diff=[[1, 1], [2, 3], [4, 6], [7, 10], [11, 100], [101, 1000]]):
    """
    Difficulty-stratified coverage violation

    Paper: Uncertainty Sets for Image Classifiers using Conformal Prediction (Angelopoulos et al., 2020)
    Link: https://arxiv.org/abs/2009.14193
    
    
    Args:
        logits (torch.Tensor): the predicted logits.
        prediction_sets (torch.Tensor): the prediction sets generated by CP algorithms.
        labels (list): the ground-truth label of each samples.
        alpha (float): the user-guided confidence level.
        strata_diff (list): a coarse partitioning of the possible difficulties.

    Returns:
        2-tuple: (the difficulty-stratified coverage violation, the number of samples, the empirical coverage and size of each difficulty).
    """
    if len(prediction_sets) != len(labels):
        raise ValueError("The number of prediction sets must be equal to the number of labels.")
    if not isinstance(strata_diff, list):
        raise TypeError("strata_diff must be a list.")

    labels = labels.cpu()
    logits = logits.cpu()
    correct_array = torch.zeros(len(labels), dtype=torch.float32)
    size_array = torch.zeros(len(labels), dtype=torch.float32)
    topk = []

    for index, ele in enumerate(logits):
        I = torch.argsort(ele, descending=True)
        target = labels[index]
        topk.append((I == target).nonzero(as_tuple=True)[0].item() + 1)
        correct_array[index] = 1 if target in prediction_sets[index] else 0
        size_array[index] = len(prediction_sets[index])
    topk = torch.tensor(topk, dtype=torch.int32)

    ccss_diff = {}
    diff_violation = -1

    for stratum in strata_diff:
        temp_index = ((topk >= stratum[0]) & (topk <= stratum[1])).nonzero(as_tuple=True)[0]
        ccss_diff[str(stratum)] = {}
        ccss_diff[str(stratum)]['cnt'] = len(temp_index)
        if len(temp_index) == 0:
            ccss_diff[str(stratum)]['cvg'] = 0
            ccss_diff[str(stratum)]['sz'] = 0
        else:
            cvg = torch.mean(correct_array[temp_index]).item()
            sz = torch.mean(size_array[temp_index]).item()

            ccss_diff[str(stratum)]['cvg'] = round(cvg, 3)
            ccss_diff[str(stratum)]['sz'] = round(sz, 3)
            stratum_violation = abs(1 - alpha - cvg)
            diff_violation = max(diff_violation, stratum_violation)

    return diff_violation, ccss_diff


@METRICS_REGISTRY_CLASSIFICATION.register()
def SSCV(prediction_sets, labels, alpha, stratified_size=[[0, 1], [2, 3], [4, 10], [11, 100], [101, 1000]]):
    """
    Size-stratified coverage violation (SSCV).
    
    Paper: Uncertainty Sets for Image Classifiers using Conformal Prediction (Angelopoulos et al., 2020)
    Link : https://iclr.cc/virtual/2021/spotlight/3435
    
    Args:
        prediction_sets (list): the prediction sets generated by CP algorithms.
        labels (list): the ground-truth label of each samples.
        alpha (float): the user-guided confidence level.
        stratified_size (list): a coarse partitioning of the possible set sizes.
    
    Returns:
        Int: the value of SSCV.
    
    """
    if len(prediction_sets) != len(labels):
        raise ValueError("The number of prediction sets must be equal to the number of labels.")
    labels = labels.cpu()
    size_array = torch.zeros(len(labels), dtype=torch.float32)
    correct_array = torch.zeros(len(labels), dtype=torch.float32)
    for index, ele in enumerate(prediction_sets):
        size_array[index] = len(ele)
        correct_array[index] = 1 if labels[index] in ele else 0

    sscv = -1
    for stratum in stratified_size:
        temp_index = ((size_array >= stratum[0]) & (size_array <= stratum[1])).nonzero(as_tuple=True)[0]
        if len(temp_index) > 0:
            stratum_violation = torch.abs((1 - alpha) - torch.mean(correct_array[temp_index])).item()
            sscv = max(sscv, stratum_violation)
    return sscv

from sklearn.model_selection import train_test_split

@METRICS_REGISTRY_CLASSIFICATION.register()
def WSC(features, S, y, delta=0.1, M=1000, test_size=0.75, random_state=2020, verbose=True):
    """
    Worst-Slice Coverage (WSC).
    
     Classification with Valid and Adaptive Coverage (Romano et al., 2020)
     Paper: Classification with Valid and Adaptive Coverage
     Link : https://proceedings.neurips.cc/paper/2020/hash/244edd7e85dc81602b7615cd705545f5-Abstract.html
     Code: https://github.com/msesia/arc/tree/d80d27519f18b11e7feaf8cf0da8827151af9ce3

    
    Args:
        features (torch.Tensor): The features of input.
        S (list): The prediction sets generated by CP algorithms.
        y (torch.Tensor): The ground-truth label of each sample.
        delta (float, optional): The user-guided confidence level. Default is 0.1.
        M (int, optional): The number of random projections. Default is 1000.
        test_size (float, optional): The proportion of the dataset to include in the test split. Default is 0.75.
        random_state (int, optional): The random seed. Default is 2020.
        verbose (bool, optional): Whether to print progress. Default is True.
    
     Returns:
         Float: the value of unbiased WSV.
    
    """
    def wsc_vab(X, y, S, v, a, b):
        n = len(y)
        cover = np.array([y[i] in S[i] for i in range(n)])
        z = np.dot(X,v)
        idx = np.where((z>=a)*(z<=b))
        coverage = np.mean(cover[idx])
        return coverage
    
    X = features.cpu().numpy()
    y = y.cpu().numpy()
    
    X_train, X_test, y_train, y_test, S_train, S_test = train_test_split(X, y, S, test_size=test_size,
                                                                        random_state=random_state)
    # Find adversarial parameters
    wsc_star, v_star, a_star, b_star = calWSC(X_train, y_train, S_train, delta=delta, M=M, random_state=random_state, verbose=verbose)
    # Estimate coverage
    coverage = wsc_vab(X_test, y_test, S_test, v_star, a_star, b_star)
    return coverage
    
    
def calWSC(X, y, S, delta=0.1, M=1000, random_state=2020, verbose=True):
    rng = np.random.default_rng(random_state)

    def wsc_v(X, y, S, delta, v):
        n = len(y)
        cover = np.array([y[i] in S[i] for i in range(n)])
        z = np.dot(X,v)
        # Compute mass
        z_order = np.argsort(z)
        z_sorted = z[z_order]
        cover_ordered = cover[z_order]
        ai_max = int(np.round((1.0-delta)*n))
        ai_best = 0
        bi_best = n-1
        cover_min = 1
        for ai in np.arange(0, ai_max):
            bi_min = np.minimum(ai+int(np.round(delta*n)),n-1)
            coverage = np.cumsum(cover_ordered[ai:n]) / np.arange(1,n-ai+1)
            coverage[np.arange(0,bi_min-ai)]=1
            bi_star = ai+np.argmin(coverage)
            cover_star = coverage[bi_star-ai]
            if cover_star < cover_min:
                ai_best = ai
                bi_best = bi_star
                cover_min = cover_star
        return cover_min, z_sorted[ai_best], z_sorted[bi_best]

    def sample_sphere(n, p):
        v = rng.normal(size=(p, n))
        v /= np.linalg.norm(v, axis=0)
        return v.T

    V = sample_sphere(M, p=X.shape[1])
    wsc_list = [[]] * M
    a_list = [[]] * M
    b_list = [[]] * M
    from tqdm import tqdm
    if verbose:
        for m in tqdm(range(M)):
            wsc_list[m], a_list[m], b_list[m] = wsc_v(X, y, S, delta, V[m])

    else:
        for m in range(M):
            wsc_list[m], a_list[m], b_list[m] = wsc_v(X, y, S, delta, V[m])                
        
    idx_star = np.argmin(np.array(wsc_list))
    a_star = a_list[idx_star]
    b_star = b_list[idx_star]
    v_star = V[idx_star]
    wsc_star = wsc_list[idx_star]
    return wsc_star, v_star, a_star, b_star


class Metrics:

    def __call__(self, metric) -> Any:
        if metric not in METRICS_REGISTRY_CLASSIFICATION.registered_names():
            raise NameError(f"The metric: {metric} is not defined in TorchCP.")
        return METRICS_REGISTRY_CLASSIFICATION.get(metric)
