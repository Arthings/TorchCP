# Copyright (c) 2023-present, SUSTech-ML.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#

import numpy as np
from typing import Any

from torchcp.utils.registry import Registry

METRICS_REGISTRY_CLASSIFICATION = Registry("METRICS")

#########################################
# Marginal coverage metric
#########################################

@METRICS_REGISTRY_CLASSIFICATION.register()
def coverage_rate(prediction_sets, labels, coverage_type="default", num_classes=None):
    """
    The metric for empirical coverage.
    
    Empirically Validating Conformal Prediction on Modern Vision Architectures Under Distribution Shift and Long-tailed Data (Kasa et al., 2023)
    paper: https://arxiv.org/abs/2307.01088
    
    Args:
        prediction_sets (List): the prediction sets generated by CP algorithms.
        labels (list): the ground-truth label of each samples.
        coverage_type (str, optional): the type of coverage rate. Defaults to "default". Options are 'default' (the marginal coverage rate), 'macro' (the average coverage rate of all classes).
        num_classes (_type_, optional): the number of classes. When coverage_type == 'macro", you must define the number of classes.

    Returns:
        float: the empirical coverage rate.
    """
    assert len(prediction_sets) > 0, "The number of prediction set must be greater than 0."
    labels = labels.cpu()
    cvg = 0

    if coverage_type == "macro":
        assert (num_classes != None), "Macro Coverage metric needs the number of classes."
        rate_classes = []
        for k in range(num_classes):
            idx = np.where(labels == k)[0]
            selected_preds = [prediction_sets[i] for i in idx]
            if len(labels[labels == k]) != 0:
                rate_classes.append(coverage_rate(selected_preds, labels[labels == k]))
            else:
                # If there is no the "k" class in the "labels", we skip the calculation of this class.
                continue
        cvg = np.mean(rate_classes)
    else:
        for index, ele in enumerate(zip(prediction_sets, labels)):
            if ele[1] in ele[0]:
                cvg += 1
        cvg = cvg / len(prediction_sets)
    return cvg


@METRICS_REGISTRY_CLASSIFICATION.register()
def average_size(prediction_sets, labels):
    assert len(prediction_sets) > 0, "The number of prediction set must be greater than 0."

    labels = labels.cpu()
    avg_size = 0
    for index, ele in enumerate(prediction_sets):
        avg_size += len(ele)
    return avg_size / len(prediction_sets)


class Metrics:

    def __call__(self, metric) -> Any:
        if metric not in METRICS_REGISTRY_CLASSIFICATION.registered_names():
            raise NameError(f"The metric: {metric} is not defined in TorchCP.")
        return METRICS_REGISTRY_CLASSIFICATION.get(metric)